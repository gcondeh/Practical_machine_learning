---
title: "Practical Machine Learning (Jonh Hopkins)"
author: "GCH (+Google translate)"
date: "20-11-2023"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(patchwork)

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")


```

# Aim

Classification and prediction of the quality of physical exercise based on data obtained from devices that monitor activity. The data has been collected and shared by [Human Activity Recognition](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har), specifically those related to the weight lifting, obtained from accelerometers located on the belt, forearm, arm and dumbbell of 6 participants.

The exercises were performed in 5 different ways:

- According to specification (Class A)

- Bringing the elbows forward (Class B)

- Lifting the dumbbell only halfway (Class C)

- Lowering the dumbbell only halfway (Class D)

- Throwing the hip forward (Class E).

A supervised classification algorithm will be used on the training data provided to generate a model that allows us to identify the quality of the exercise performed from the data provided by the devices during the exercise.


# A first look at the data

We perform a first inspection of the data to detect anomalies and biases.

```{r vista_incial, }
str(training[c(1:13,length(training))])

```

We graphically see a sample of relationships between variables.

```{r graf_dependecias, echo=FALSE}

training %>%
  mutate(classe = as.factor(classe)) %>%
  mutate(user_name = as.factor(user_name)) %>%
  mutate(cvtd_timestamp = dmy_hm(cvtd_timestamp)) -> training

featurePlot(x = training[,c(1,3,7)], 
            y = training$classe, 
            plot = "pairs",
            ## Add a key at the top
             auto.key = list(columns = 5))

```

It seems necessary to purge the data set as follows:

- Some columns will be factors: username, classe

- The observations are ordered by class and timestamp, making the best indicator to classify the date, something meaningless. We remove references to the order: X, window, timestamps, ...

- Some communes are empty, are only reported in some cases or have erroneous data (DIV0). We will remove them, leaving only factorial and numerical variables.


# Model selection


The model we will use will be Random Forest. It allows the classification of a large number of predictor variables, with high precision, low variance and little overfitting. The information it provides on the importance of the variables will give us an idea of ​​the characteristics of the model. As a negative we will have the execution time, acceptable with the volume of data that we will use.

We will check the results with a test data set that will be 30% of the data provided, chosen randomly on the Classe variable and verifying a consistent distribution of this variable with the training data.


```{r prep_datos_1 , echo=FALSE}
which(colMeans(is.na(training))>0) -> quitar
which(sapply(training, class) == "character")  -> quitar_2
quitar_3 <- c(1, 3:5, 7)
training[, -c(quitar, quitar_2, quitar_3)]-> training_wk

```

```{r prep_datos_2, echo=TRUE}
set.seed(2004)
inTrain = createDataPartition(training_wk$classe, p = 0.7, list = FALSE)
training_wk_entr = training_wk[ inTrain,]
training_wk_test = training_wk[-inTrain,]

```

```{r prep_datos_4, echo=FALSE}
nrow(training_wk_entr) -> todo
training_wk_entr %>%
  group_by(classe) %>%
  summarise( n(), n()/todo) -> graf_1

ggplot(graf_1, aes(x=classe, y=`n()` , fill=classe))+
  geom_col() +
  geom_text(label = round(graf_1$`n()/todo`,2), vjust = -0.5)+
  ggtitle("Training distribution\n")+
  theme_classic()+
  theme(legend.position = "none", axis.title = element_blank())-> p11
  
nrow(training_wk_test) -> todo
training_wk_test %>%
  group_by(classe) %>%
  summarise( n(), n()/todo) -> graf_2

ggplot(graf_2, aes(x=classe, y=`n()` , fill=classe))+
  geom_col() +
  geom_text(label = round(graf_2$`n()/todo`,2), vjust = -0.5) +
  ggtitle("Distribución test\n")+
  theme_classic()+
  theme(legend.position = "none", axis.title = element_blank())-> p12

p11+p12

```


We will improve the accuracy by using the K-fold cross-validation method (k=10).


```{r ejecucion_modelo, echo=TRUE}

set.seed(2212)
modelo_rf <- train(classe ~ ., method = "rf", data = training_wk_entr, trControl = trainControl(method = "cv", number = 10))

modelo_rf

```



# Results
  
The precision we obtain is greater than 99%, using 29 out of 53 predictors and 500 decision trees.
  

```{r resultado_1}

modelo_rf$final

```



We can see graphically how the error evolves with the number of trees that make up the model. Data that we could eventually use to tune the model.



```{r resultado_2, echo = FALSE}

plot(modelo_rf$finalModel, main="Error depending on the number of trees used")

```


Now we check our model on the test data, which we had previously set aside, and thus verify the reliability of what was done. The result obtained maintains a precision greater than 99%, aligned with the data we already had and making us think that it is too bad we did not do it.


```{r resultado_3}
predic_rf <- predict(modelo_rf, newdata = training_wk_test)

confusionMatrix(predic_rf, training_wk_test$classe)
```



Finally we review the importance of the predictors when making the estimates. It is striking that the main predictors refer to the Euler angles of the belt (how the orientation of the belt varies over its own reference system) and the data from the dumbbell accelerometer.


```{r importancia_var, echo=FALSE}
plot(varImp(modelo_rf), top = 10, main = "Top Predictors by importance")

```
  

We reviewed the relationship of the most important predictors with each other to identify possible biases that we were unable to appreciate. I don't see anything striking.
  
 
```{r importancia_var_2, echo=FALSE}
featurePlot(x = training_wk_entr[,c(2,4,42,40)], 
            y = training_wk_entr$classe, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 5)) 
```


